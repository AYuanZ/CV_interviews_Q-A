## 问题

在上一个问题 “11_boosting思想” 中我们已经简单谈了下提升方法 boosting 的基本思路，这个问题让我们深入了解下 boosting 思想中最具代表性的算法 AdaBoost。

## AdaBoost算法

**特点：**

1. 不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用
2. 利用基本分类器的线性组合构建最终的分类器

假设给定一个二类分类的训练数据集：
$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
其中每个样本点由实例与标记组成。实例 $x_i \in \chi \subseteq R^n$ ，标记 $y_i \in \Upsilon = \{-1,1\}$ ，$\chi$ 是实例空间，$\Upsilon$ 是标记集合。Adaboost 利用以下算法，从训练数据中学习一系列弱分类器，并将这些弱分类器线性组合成为一个强分类器。

(一) 初始化训练数据的权值分布：
$$
D_1 = (w_{11},...,w_{1i},...,w_{1N}),\quad w_{1i}=\frac{1}{N},\quad i=1,2,...,N
$$

> 假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同。这一假设保证第１步能够在原始数据上学习基本分类器 $G_1(x)$ 。

(二) 一共需要学习 M 个基本分类器，则在每一轮 $m = 1,2,...,M$ 顺次地执行下列操作：

1.  **使用当前分布 $D_m$ 加权的训练数据集，得到基本分类器：**
   $$
   G_m(x):\chi\rightarrow \{-1,+1\}
   $$

2. **计算 $G_m(x)$ 在训练数据集上的分类误差率：**
   $$
   e_m = \sum_{i=1}^{N}P(G_m(x_i)\not = y_i) = \sum_{G_{mi}\not = y_i}w_{mi}
   $$
   $w_{mi}$ 表示第 m 轮中第 i 个实例的权值，$\sum_{i=1}^{N}w_{mi} = 1$ 。这表明，  $G_{m}(x)$  在加权的训练数据集上的分类误差率是被 $G_{m}(x)$  误分类的样本的权值之和。

3. **计算 $G_{m}(x)$ 的系数：**
   $$
   \alpha_m = \frac{1}{2}ln \frac{1-e_m}{e_m}
   $$
   $\alpha_m$ 表示 $G_{m}(x)$ 在最终分类器中的重要性。根据式子可知，$e_m \le \frac{1}{2}$ 时，$\alpha_m \ge 0$ ，并且 $\alpha_m$ 随着 $e_m$ 的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。**这里注意所有 $\alpha_m$ 加起来的和并不等于 1 。** （<u>注意 $e_m$ 是有可能比 $\frac{1}{2}$ 大的，也就是说 $\alpha_m$ 有可能小于 0，《统计学习方法》没有讨论这种情况的处理方法，但在西瓜书中的处理方法是抛弃该分类器，且终止学习过程，哪怕学习到的分类器个数远远没有达到预设的 M</u>）

4. **更新训练数据集的权值分布：**
   $$
   D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N}) \\
   w_{m+1,i} = \frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)), \quad i=1,2,...,N \\
   其中 Z_m是规范因子，Z_m = \sum_{i=1}^{N}w_{mi}exp(-\alpha_my_iG_m(x_i))
   $$
   $w_{m+1,i}$ 也可以写成分段函数的形式：
   $$
   w_{m+1,i}= \begin {cases} 
   \frac{w_{mi}}{Z_m}e^{-\alpha_m}, & G_m(x_i) = y_i \\
   \frac{w_{mi}}{Z_m}e^{\alpha_m}, & G_m(x_i) \not = y_i
   \end {cases}
   $$
   也就是说被基本分类器 $G_m(x)$ 误分类样本的权值得以增大，而被正确分类的样本的权值却变小。两者相比可知误分类样本的权值被放大 $e^{2\alpha_m} = \frac{1-e_m}{e_m}$ 倍。因此，误分类样本在下一轮学习中起更大的作用。

(三) 经过以上过程后可以得到 M 个基本分类器，构建基本分类器的线性组合：
$$
f(x) = \sum_{m=1}^{M}\alpha_m G_m(x)
$$
得到最终分类器：
$$
G(x) = sign(f(x)) = sign(\sum_{m=1}^{M}\alpha_m G_m(x))
$$
线性组合 $f(x)$ 实现 M 个基本分类器的加权表决，$f(x)$ 的符号决定了实例 $x$ 的类，$f(x)$ 的绝对值表示分类的确信度。

不得不说，整个算法的设计很巧妙！

引自《机器学习》

> 对无法接受带权样本的基分类器学习方法，则可通过“重采样法”来处理，即在每一轮学习中，根据样本分布对训练集重新采样，再用重采样而得的样本集对基分类器进行训练。上面也说到了，AdaBoost模型学习过程中，当出现一个基分类器的误分类误差大于 0.5 即比随机猜测还要差时，处理方法是将该分类器丢弃，且终止学习过程，此种情形下，初始设置的学习轮次 M 也许还未达到，可能会导致最终集成中只包含很少的基分类器而性能不佳。若采用“重采样法”，则可获得“重启动”机会以避免训练过程过早停止，即在抛弃不满足条件的的当前基分类器之后，可根据当前分布重新对训练样本进行采样，再基于新的采样结果重新训练出基分类器，从而使得学习过程可以持续到预设的 M 轮完成。

## Adaboost 的另一种解释

> 可以认为 AdaBoost 算法是模型为加法模型、损失函数为指数函数、学习算法为前向分布算法时的二类分类学习方法。

首先需要介绍一下**前向分布算法：**

假设加法模型：
$$
f(x) = \sum_{m=1}^{M}\beta_mb(x;\gamma_m)
$$
其中，$b(x;\gamma_m)$ 为基函数，$\gamma_m$ 为基函数的参数，$\beta_m$ 为基函数的系。

在给定训练数据即损失函数 $L(y,f(x))$ 的条件下，学习加法模型 $f(x)$ 成为经验风险极小化即损失函数极小化问题：
$$
min_{\beta_m,\gamma_m}\sum_{i=1}^{N}L(y_i,\sum_{m=1}^{M}\beta_mb(x_i;\gamma_m))
$$
这是一个复杂的优化问题，前向分布算法求解这一问题的想法是：因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数，逐步逼近以上优化函数式，那么就可以简化优化的复杂度。具体地，每一步只需要优化如下损失函数：
$$
min_{\beta,\gamma}\sum_{i=1}^{N}L(y_i,\beta b(x_i;\gamma))
$$
**下面使用前向分步算法推导出 AdaBoost**

此时的模型是由基本分类器组成的加法模型：
$$
f(x ) = \sum_{m=1}^{M}\alpha_mG_m(x)
$$
其损失函数为**指数损失函数**：$L(y,f(x)) = exp(-yf(x))$ (书中还用了一大段来证明前向分步算法的损失函数是指数损失函数)

在第 m 轮迭代中可以得到 $\alpha_m$，$G_m(x)$ 和 $f_m(x)$。
$$
f_m(x) = f_{m-1}(x)+\alpha_mG_m(x)
$$
目标是使前向分步算法得到的 $\alpha_m$ 和 $G_m(x)$ 使 $f_m(x)$ 在训练集上的指数损失最小，即：
$$
(\alpha_m,G_m(x)) = arg min_{\alpha,G}\sum_{i=1}^{N}exp[-y_i(f_{m-1}(x_i)+\alpha G(x_i))]
$$
也可以表示为：
$$
(\alpha_m,G_m(x)) = arg min_{\alpha,G}\sum_{i=1}^{N}\hat{w}_{mi}exp[-y_i\alpha G(x_i)]
$$
其中 $\hat{w}_{mi}=exp[-y_if_{m-1}(x_i)]$ 。因为 $\hat{w}_{mi}$ 既不依赖于 $\alpha$ 也不依赖于 G ，所以与最小化无关。但 $\hat{w}_{mi}$ 依赖于 $f_{m-1}(x)$ ，随着每一轮迭代而发生改变。

可以证明使上面式子达到最小的 $\alpha_m^*$ 和 $G_m^*(x)$ 就是 AdaBoost 算法所得到的 $\alpha_m$ 和 $G_m(x)$ 。