## 问题

随机森林算法思想，怎么增加随机性，如何评估特征重要性，为什么不容易过拟合

## 随机森林思想怎么添加的随机性

随机森林 (RF) 是 Bagging 的一个变体。RF在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入随机性：

> 传统决策树在选择划分属性时，是在当前结点的属性集合（假定有 d 个属性）中选择一个最优属性；而在 RF 中，对基决策树的每一个结点，**先从结点的属性集合中随机选择一个包含 k 个属性的子集**，然后再从这个子集当中选择一个最优属性用于划分。

这里的参数 k 控制了随机性的引入程度。若令 $k=d$ ，则基决策树的构建与传统决策树相同，一般情况下，推荐值为 $k=log_2 d$ 。

## 为什么不容易过拟合

因为随机森林中每棵树的训练样本是随机的，每棵树中的每个结点的分裂属性也是随机选择的。这两个随机性的引入，使得随机森林不容易陷入过拟合。且树的数量越多，随机森林通常会收敛到更低的泛化误差。理论上当树的数目趋于无穷时，随机森林便不会出现过拟合，但是现实当做做不到训练无穷多棵树。

## 如何评估特征的重要性

这个问题是决策树的核心问题，而随机森林是以决策树为基学习器的，所以这里大概提提，详细的可以去看看决策树模型。

决策树中，根节点包含样本全集，其他非叶子结点包含的样本集合根据选择的属性被划分到子节点中，叶节点对应于分类结果。决策树的关键是在非叶子结点中怎么选择最优的属性特征以对该结点中的样本进行划分，方法主要有**信息增益、增益率以及基尼系数**３种，三种方法的定义还是有点复杂的，且没有例子的话不好理解，因此这里就不把书上的东西搬上来了，建议去查阅西瓜书的 $p_{75}-p_{79}$ 。

